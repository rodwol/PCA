{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodwol/PCA/blob/main/PCA_Formative_1%5BPeer_Group7%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkivgEHr_KW"
      },
      "source": [
        "# Formative Assignment: Advanced Linear Algebra (PCA)\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "This notebook will guide you through the implementation of Principal Component Analysis (PCA). Fill in the missing code and provide the required answers in the appropriate sections. You will work with the `fuel_econ.csv` dataset.\n",
        "\n",
        "Make sure to display outputs for each code cell when submitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xycIp758r_Kb"
      },
      "source": [
        "### Step 1: Load and Standardize the Data\n",
        "Before applying PCA, we must standardize the dataset. Standardization ensures that all features have a mean of 0 and a standard deviation of 1, which is essential for PCA.\n",
        "Fill in the code to standardize the dataset.\n",
        "\n",
        "STRICTLY - Write code that implements standardization based on the image below\n",
        "\n",
        "<img src='data:image/jpeg;base64,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'/>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/rodwol/PCA/main/owid-covid-latest.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "numeric_data = data.select_dtypes(include=['number'])\n",
        "numeric_data = numeric_data.dropna(axis=1, how='any')\n",
        "numeric_data = numeric_data.fillna(numeric_data.mean())\n",
        "numeric_data = numeric_data.loc[:, numeric_data.std() > 0]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "standardized = scaler.fit_transform(numeric_data)\n",
        "\n",
        "print(numeric_data.head())\n",
        "\n",
        "print(\"Shape of numeric data:\", numeric_data.shape)\n",
        "print(\"Any NaNs left?\", numeric_data.isna().sum().sum() > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VusihsJUKlcb",
        "outputId": "28af3b0b-d122-4544-8469-d5e524c5c867"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     population\n",
            "0  4.112877e+07\n",
            "1  1.426737e+09\n",
            "2  2.842318e+06\n",
            "3  4.490323e+07\n",
            "4  4.429500e+04\n",
            "Shape of numeric data: (247, 1)\n",
            "Any NaNs left? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKihXBaBr_Kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d671b9f-3c67-48c2-d3a4-e4d470a9bb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any NaNs in standardized? total_cases                              False\n",
            "new_cases                                False\n",
            "new_cases_smoothed                       False\n",
            "total_deaths                             False\n",
            "new_deaths                               False\n",
            "new_deaths_smoothed                      False\n",
            "total_cases_per_million                  False\n",
            "new_cases_per_million                    False\n",
            "new_cases_smoothed_per_million           False\n",
            "total_deaths_per_million                 False\n",
            "new_deaths_per_million                   False\n",
            "new_deaths_smoothed_per_million          False\n",
            "icu_patients                             False\n",
            "icu_patients_per_million                 False\n",
            "hosp_patients                            False\n",
            "hosp_patients_per_million                False\n",
            "weekly_icu_admissions                    False\n",
            "weekly_icu_admissions_per_million        False\n",
            "weekly_hosp_admissions                   False\n",
            "weekly_hosp_admissions_per_million       False\n",
            "total_vaccinations                       False\n",
            "people_vaccinated                        False\n",
            "people_fully_vaccinated                  False\n",
            "total_boosters                           False\n",
            "new_vaccinations                         False\n",
            "new_vaccinations_smoothed                False\n",
            "total_vaccinations_per_hundred           False\n",
            "people_vaccinated_per_hundred            False\n",
            "people_fully_vaccinated_per_hundred      False\n",
            "total_boosters_per_hundred               False\n",
            "new_vaccinations_smoothed_per_million    False\n",
            "new_people_vaccinated_smoothed           False\n",
            "population_density                       False\n",
            "median_age                               False\n",
            "aged_65_older                            False\n",
            "aged_70_older                            False\n",
            "gdp_per_capita                           False\n",
            "extreme_poverty                          False\n",
            "cardiovasc_death_rate                    False\n",
            "diabetes_prevalence                      False\n",
            "female_smokers                           False\n",
            "male_smokers                             False\n",
            "handwashing_facilities                   False\n",
            "hospital_beds_per_thousand               False\n",
            "life_expectancy                          False\n",
            "human_development_index                  False\n",
            "population                               False\n",
            "dtype: bool\n",
            "Any Infs in standardized? total_cases                              False\n",
            "new_cases                                False\n",
            "new_cases_smoothed                       False\n",
            "total_deaths                             False\n",
            "new_deaths                               False\n",
            "new_deaths_smoothed                      False\n",
            "total_cases_per_million                  False\n",
            "new_cases_per_million                    False\n",
            "new_cases_smoothed_per_million           False\n",
            "total_deaths_per_million                 False\n",
            "new_deaths_per_million                   False\n",
            "new_deaths_smoothed_per_million          False\n",
            "icu_patients                             False\n",
            "icu_patients_per_million                 False\n",
            "hosp_patients                            False\n",
            "hosp_patients_per_million                False\n",
            "weekly_icu_admissions                    False\n",
            "weekly_icu_admissions_per_million        False\n",
            "weekly_hosp_admissions                   False\n",
            "weekly_hosp_admissions_per_million       False\n",
            "total_vaccinations                       False\n",
            "people_vaccinated                        False\n",
            "people_fully_vaccinated                  False\n",
            "total_boosters                           False\n",
            "new_vaccinations                         False\n",
            "new_vaccinations_smoothed                False\n",
            "total_vaccinations_per_hundred           False\n",
            "people_vaccinated_per_hundred            False\n",
            "people_fully_vaccinated_per_hundred      False\n",
            "total_boosters_per_hundred               False\n",
            "new_vaccinations_smoothed_per_million    False\n",
            "new_people_vaccinated_smoothed           False\n",
            "population_density                       False\n",
            "median_age                               False\n",
            "aged_65_older                            False\n",
            "aged_70_older                            False\n",
            "gdp_per_capita                           False\n",
            "extreme_poverty                          False\n",
            "cardiovasc_death_rate                    False\n",
            "diabetes_prevalence                      False\n",
            "female_smokers                           False\n",
            "male_smokers                             False\n",
            "handwashing_facilities                   False\n",
            "hospital_beds_per_thousand               False\n",
            "life_expectancy                          False\n",
            "human_development_index                  False\n",
            "population                               False\n",
            "dtype: bool\n",
            "Standardized shape: (247, 47)\n"
          ]
        }
      ],
      "source": [
        "def standardize_data(data):\n",
        "    data = data.select_dtypes(include=[np.number])\n",
        "    data = data.dropna(axis=1, how='all')\n",
        "    data = data.fillna(data.mean())\n",
        "    std = data.std()\n",
        "    data = data.loc[:, std > 0]\n",
        "\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "\n",
        "    standardized_data = (data - mean) / std\n",
        "    return standardized_data\n",
        "\n",
        "standardized = standardize_data(data)\n",
        "print(\"Any NaNs in standardized?\", np.isnan(standardized).any())\n",
        "print(\"Any Infs in standardized?\", np.isinf(standardized).any())\n",
        "print(\"Standardized shape:\", standardized.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fybn40Syr_Kd"
      },
      "source": [
        "### Step 3: Calculate the Covariance Matrix\n",
        "The covariance matrix helps us understand how the features are related to each other. It is a key component in PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RbklA9tqr_Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3a8750-0de7-4b22-ce45-65831791c128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance matrix shape: (1, 1)\n",
            "Any NaNs? False\n",
            "Any infs? False\n",
            "[[1.]]\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Calculate the Covariance Matrix\n",
        "def compute_covariance_matrix(X_std):\n",
        "    # X_std is already a NumPy array, so no need to convert\n",
        "    n_samples = X_std.shape[0]\n",
        "    # Ensure n_samples - 1 is not zero to avoid division by zero\n",
        "    if n_samples <= 1:\n",
        "        raise ValueError(\"Cannot compute covariance matrix with 1 or fewer samples.\")\n",
        "    cov_matrix = (X_std.T @ X_std) / (n_samples - 1)  # Use n - 1\n",
        "    return cov_matrix\n",
        "\n",
        "cov_matrix = compute_covariance_matrix(standardized)\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Covariance matrix shape:\", cov_matrix.shape)\n",
        "print(\"Any NaNs?\", np.isnan(cov_matrix).any())\n",
        "print(\"Any infs?\", np.isinf(cov_matrix).any())\n",
        "print(cov_matrix.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWzqXsR0r_Ke"
      },
      "source": [
        "### Step 4: Perform Eigendecomposition\n",
        "Eigendecomposition of the covariance matrix will give us the eigenvalues and eigenvectors, which are essential for PCA.\n",
        "Fill in the code to compute the eigenvalues and eigenvectors of the covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Tm0rzdAr_Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d8bc19-bfb5-4ba7-ab84-d331c4a1ec18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues shape: (1,)\n",
            "Eigenvectors shape: (1, 1)\n",
            "\n",
            "First 5 eigenvalues: [1.00406504]\n",
            "\n",
            "Sum of eigenvalues: 1.0040650406504064\n",
            "\n",
            "All eigenvalues:\n",
            "[1.00406504]\n",
            "\n",
            "First eigenvector (first column):\n",
            "[1.]\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Perform Eigendecomposition\n",
        "import numpy as np\n",
        "# Ensure the previous cell (Step 3) calculating cov_matrix has been executed\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Eigenvalues shape:\", eigenvalues.shape)\n",
        "print(\"Eigenvectors shape:\", eigenvectors.shape)\n",
        "print(\"\\nFirst 5 eigenvalues:\", eigenvalues[:5])\n",
        "print(\"\\nSum of eigenvalues:\", np.sum(eigenvalues))\n",
        "\n",
        "# Display eigenvalues and eigenvectors\n",
        "print(\"\\nAll eigenvalues:\")\n",
        "print(eigenvalues)\n",
        "print(\"\\nFirst eigenvector (first column):\")\n",
        "print(eigenvectors[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp6o_4sTr_Kf"
      },
      "source": [
        "### Step 5: Sort Principal Components\n",
        "Sort the eigenvectors based on their corresponding eigenvalues in descending order. The higher the eigenvalue, the more important the eigenvector.\n",
        "Complete the code to sort the eigenvectors and print the sorted components.\n",
        "\n",
        "<a url ='https://www.youtube.com/watch?v=vaF-1xUEXsA&t=17s'>How Is Explained Variance Used In PCA?'<a/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpfoRz-Xr_Kf",
        "outputId": "1ea80a98-0de0-426e-ae10-8f63ae28821d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1) (<ipython-input-1-405520831>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-405520831>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Of course. Let's start over from a clean slate.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ],
      "source": [
        "Of course. Let's start over from a clean slate.\n",
        "\n",
        "I will provide a single, complete block of Python code that you can run in your Jupyter notebook. I will break it down into the numbered steps you provided, adding detailed explanations in the code comments so you can understand what's happening at each stage.\n",
        "\n",
        "We will perform the entire PCA process from scratch using NumPy, addressing the division by zero error you encountered right at the beginning.\n",
        "\n",
        "The Complete PCA Process from Scratch\n",
        "\n",
        "Here is the full script. Copy and paste this into a single cell in your Jupyter notebook and run it.\n",
        "\n",
        "# =============================================================================\n",
        "# PRE-REQUISITES: Import Libraries\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 0: LOAD AND CLEAN THE DATA\n",
        "#\n",
        "# EXPLANATION:\n",
        "# PCA only works on a clean, numerical dataset. Real-world data is messy.\n",
        "# We must perform three crucial cleaning steps first:\n",
        "#   1. Select only numeric columns (PCA can't handle text like 'USA').\n",
        "#   2. Fill missing values (NaNs), as PCA can't handle them. We'll use the mean.\n",
        "#   3. **(THE FIX)** Remove columns that have ZERO variance. A column where all\n",
        "#      values are the same has a standard deviation of 0. Trying to standardize\n",
        "#      this column causes a \"division by zero\" error. These columns are also\n",
        "#      useless for PCA, as they contain no information about differences\n",
        "#      in the data.\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 0: Loading and Cleaning Data ---\")\n",
        "\n",
        "# Load data from the URL\n",
        "url = 'https://raw.githubusercontent.com/rodwol/PCA/main/owid-covid-latest.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# 1. Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "# 2. Fill missing values with the mean of each column\n",
        "numeric_df_filled = numeric_df.fillna(numeric_df.mean())\n",
        "\n",
        "# 3. Find and remove columns with zero standard deviation (zero variance)\n",
        "#    This is the critical step to prevent the \"division by zero\" warning.\n",
        "variances = numeric_df_filled.var()\n",
        "non_zero_variance_cols = variances[variances > 0].index\n",
        "data_cleaned = numeric_df_filled[non_zero_variance_cols]\n",
        "\n",
        "# Keep track of the final column names for plotting later\n",
        "final_column_names = list(data_cleaned.columns)\n",
        "\n",
        "# Convert our clean pandas DataFrame to a NumPy array for calculations\n",
        "X = data_cleaned.values\n",
        "\n",
        "print(f\"Original numeric columns: {len(numeric_df.columns)}\")\n",
        "print(f\"Columns after removing zero-variance ones: {len(final_column_names)}\")\n",
        "print(f\"Final data shape for PCA: {X.shape}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: STANDARDIZE THE DATA\n",
        "#\n",
        "# EXPLANATION:\n",
        "# As your image shows, we rescale each feature (column) so it has a\n",
        "# mean (μ) of 0 and a standard deviation (σ) of 1. This is vital because\n",
        "# features with large values (like 'population') would otherwise dominate\n",
        "# the PCA process over features with small values (like 'positive_rate').\n",
        "# We use the formula: z = (x - μ) / σ\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 1: Standardizing Data ---\")\n",
        "\n",
        "# Calculate the mean (μ) of each column (axis=0 means column-wise)\n",
        "data_mean = np.mean(X, axis=0)\n",
        "\n",
        "# Calculate the standard deviation (σ) of each column\n",
        "data_std = np.std(X, axis=0)\n",
        "\n",
        "# Apply the standardization formula\n",
        "standardized_data = (X - data_mean) / data_std\n",
        "\n",
        "# Check: The mean of each column should now be very close to 0\n",
        "# and the standard deviation should be very close to 1.\n",
        "print(\"Standardization complete.\")\n",
        "# print(f\"Mean after standardization (should be ~0): {np.mean(standardized_data, axis=0)}\")\n",
        "# print(f\"Std Dev after standardization (should be ~1): {np.std(standardized_data, axis=0)}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: CALCULATE THE COVARIANCE MATRIX\n",
        "#\n",
        "# EXPLANATION:\n",
        "# The covariance matrix is a square table that describes the relationships\n",
        "# between all pairs of features.\n",
        "#   - A positive value means two features tend to increase together.\n",
        "#   - A negative value means one increases as the other decreases.\n",
        "#   - A value near zero means they have little linear relationship.\n",
        "# This matrix captures the total variance and structure of the data.\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 2: Calculating Covariance Matrix ---\")\n",
        "\n",
        "# 'rowvar=False' is crucial. It tells NumPy that our features are in COLUMNS,\n",
        "# not rows.\n",
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "print(f\"Shape of covariance matrix: {cov_matrix.shape}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: PERFORM EIGENDECOMPOSITION\n",
        "#\n",
        "# EXPLANATION:\n",
        "# This is the mathematical core of PCA. We break down the covariance matrix\n",
        "# into two parts:\n",
        "#   - Eigenvectors: These are the directions of the new feature axes, known\n",
        "#     as the \"principal components.\" They point in the directions of maximum\n",
        "#     variance in the data.\n",
        "#   - Eigenvalues: These numbers tell us the *amount* of variance captured\n",
        "#     by each corresponding eigenvector. A big eigenvalue means its\n",
        "#     eigenvector is very important.\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 3: Performing Eigendecomposition ---\")\n",
        "\n",
        "# np.linalg.eigh is a specialized, stable function for symmetric matrices\n",
        "# like our covariance matrix.\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "print(\"Eigendecomposition complete.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: SORT PRINCIPAL COMPONENTS\n",
        "#\n",
        "# EXPLANATION:\n",
        "# We want to rank the principal components (eigenvectors) from most to least\n",
        "# important. We do this by sorting the eigenvalues in descending order and\n",
        "# rearranging the eigenvectors to match that new order.\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 4: Sorting Principal Components ---\")\n",
        "\n",
        "# Get the indices that would sort the eigenvalues in descending order\n",
        "# (argsort gives ascending, so we reverse it with [::-1])\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "\n",
        "# Sort the eigenvalues and eigenvectors based on these indices\n",
        "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "print(\"Eigenvectors sorted by importance.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: PROJECT DATA ONTO PRINCIPAL COMPONENTS\n",
        "#\n",
        "# EXPLANATION:\n",
        "# This is the actual \"dimensionality reduction\" step. We decide how many\n",
        "# new dimensions (principal components) we want to keep. For visualization,\n",
        "# we'll keep the top 2. We then transform our original standardized data\n",
        "# into this new, smaller space using matrix multiplication (the dot product).\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 5: Projecting Data ---\")\n",
        "\n",
        "# Decide how many principal components to keep (e.g., 2 for a 2D plot)\n",
        "num_components = 2\n",
        "\n",
        "# Get the top 'num_components' from our sorted eigenvectors\n",
        "top_components = sorted_eigenvectors[:, :num_components]\n",
        "\n",
        "# Project the standardized data onto these components\n",
        "reduced_data = np.dot(standardized_data, top_components)\n",
        "\n",
        "print(f\"Data has been reduced from {X.shape[1]} dimensions to {num_components}.\")\n",
        "print(f\"Shape of the new, reduced data: {reduced_data.shape}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: VISUALIZE BEFORE AND AFTER PCA\n",
        "#\n",
        "# EXPLANATION:\n",
        "# Let's plot the data to see the results.\n",
        "#   - The \"Before\" plot shows two of the original features.\n",
        "#   - The \"After\" plot shows our new, 2-dimensional data. You'll see the\n",
        "#     data is now oriented along the axes of maximum variance.\n",
        "# =============================================================================\n",
        "print(\"\\n--- Step 6: Visualizing Results ---\")\n",
        "\n",
        "# Create a figure to hold our two plots\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "# --- PLOT 1: ORIGINAL DATA (BEFORE PCA) ---\n",
        "plt.subplot(1, 2, 1)\n",
        "# Let's pick two interesting original features to plot against each other\n",
        "# We use the 'final_column_names' list we saved in Step 0\n",
        "idx_gdp = final_column_names.index('gdp_per_capita')\n",
        "idx_life = final_column_names.index('life_expectancy')\n",
        "plt.scatter(X[:, idx_gdp], X[:, idx_life], alpha=0.6, c='blue')\n",
        "plt.title('Before PCA: Original Data', fontsize=16)\n",
        "plt.xlabel('GDP per Capita (Original)', fontsize=12)\n",
        "plt.ylabel('Life Expectancy (Original)', fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "# --- PLOT 2: REDUCED DATA (AFTER PCA) ---\n",
        "plt.subplot(1, 2, 2)\n",
        "# The reduced data has two columns: PC1 and PC2\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], alpha=0.6, c='orange')\n",
        "plt.title('After PCA: Reduced to 2 Dimensions', fontsize=16)\n",
        "plt.xlabel('Principal Component 1', fontsize=12)\n",
        "plt.ylabel('Principal Component 2', fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.suptitle('PCA Dimensionality Reduction', fontsize=20)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "print(\"\\nVisualization complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbmuO27r_Kg"
      },
      "source": [
        "### Step 6: Project Data onto Principal Components\n",
        "Now that we’ve selected the number of components, we will project the original data onto the chosen principal components.\n",
        "Fill in the code to perform the projection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubjIMAtWr_Kg"
      },
      "outputs": [],
      "source": [
        "# Step 6: Project Data onto Principal Components\n",
        "num_components = None  # Decide on the number of principal components to keep\n",
        "reduced_data = None  # Project data onto the principal components\n",
        "reduced_data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVq-b6vtr_Kg"
      },
      "source": [
        "### Step 7: Output the Reduced Data\n",
        "Finally, display the reduced data obtained by projecting the original dataset onto the selected principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D0uzRyPr_Kg"
      },
      "outputs": [],
      "source": [
        "# Step 7: Output the Reduced Data\n",
        "print(f'Reduced Data Shape: {reduced_data.shape}')  # Display reduced data shape\n",
        "reduced_data[:5]  # Display the first few rows of reduced data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1f8ROm5r_Kg"
      },
      "source": [
        "### Step 8: Visualize Before and After PCA\n",
        "Now, let's plot the original data and the data after PCA to compare the reduction in dimensions visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwuppWV-r_Kg"
      },
      "outputs": [],
      "source": [
        "# Step 8: Visualize Before and After PCA\n",
        "\n",
        "\n",
        "# Plot original data (first two features for simplicity)\n",
        "\n",
        "\n",
        "# Plot reduced data after PCA\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}